{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kyle Larson\n",
    "# File Name: OVERTRAINED.ipynb\n",
    "# License: GPL 3\n",
    "# Objective: The objective of this document is to demonstrate a simple neural network\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import pandas\n",
    "import re\n",
    "\n",
    "#This function converts date and time from 'dd-Mth-yyyy hh:mm' to 'yyyymmddhhmm'\n",
    "def integer_Date_Time(string_1):\n",
    "    months = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    string_2 =re.search(r'\\d+', string_1).group()\n",
    "    integer_date_time =int(string_2)*10000\n",
    "    for i in range(3):\n",
    "        string_1 = string_1.replace(string_2,'')\n",
    "        if(re.search(r'\\d+', string_1)):\n",
    "            string_2 = re.search(r'\\d+', string_1).group()\n",
    "        if i==0:\n",
    "            integer_date_time +=int(string_2)*100000000\n",
    "        if i==1:\n",
    "            integer_date_time +=int(string_2)*100\n",
    "        if i==2:\n",
    "            integer_date_time +=int(string_2)\n",
    "            regex = r'(?<=-)([\\w\\.-]+)- :(\\d+)'\n",
    "            match = re.search(regex, string_1)\n",
    "            if match: \n",
    "                for j in range(len(months)):\n",
    "                    if(re.match(match.group(1), months[j])):\n",
    "                        integer_date_time+=(j+1)*1000000\n",
    "    return integer_date_time\n",
    "\n",
    "#This function cleans Active Calories, Cycling Distance, Distance and Steps files\n",
    "#reading their raw data and writing it to smaller tidier files by day\n",
    "def clean_ACCDDS(data_source, i_loc, o_loc):\n",
    "    d ={ 'Time':[0] , data_source:[0] }\n",
    "    df = pandas.DataFrame(d, columns=['Time', data_source])\n",
    "    \n",
    "    T ='Time'\n",
    "    D =data_source\n",
    "    line_count =0\n",
    "    start_file_index =0\n",
    "    Nextdate =0\n",
    "    date =0\n",
    "    with open(i_loc+data_source+\".csv\", mode='r') as csv_file:\n",
    "    \n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count =0\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if(line_count>0):\n",
    "                df = df.append({T:integer_Date_Time(row[1]),D:float(row[2])}, ignore_index=True)\n",
    "                Nextdate = df.loc[line_count][0] - (df.loc[line_count][0])%10000\n",
    "                #print(str(integer_Date_Time(row[1]))+\", \"+row[2])\n",
    "                if(line_count>1 and Nextdate-date >= 10000):\n",
    "                    with open(o_loc+data_source+str(Nextdate/10000)+\".csv\", mode='w') as outf:\n",
    "                        i =start_file_index\n",
    "                        outf.write(\"Time, \"+data_source+\"\\n\")\n",
    "                        while(i<line_count):\n",
    "                            outf.write(str(df.loc[i][0])+\", \"+str(df.loc[i][1])+\"\\n\")\n",
    "                            i +=1\n",
    "                    start_file_index = line_count\n",
    "            line_count +=1\n",
    "            date = Nextdate\n",
    "    return\n",
    "\n",
    "\n",
    "#___MAIN____\n",
    "dataSource = ['\\Active Calories', '\\Cycling Distance', '\\Distance', '\\Steps']#['\\Heart Rate']\n",
    "O_loc =r'C:\\Users\\admin\\anaconda3\\01 PROJECTS\\02 HEART RATE\\By Day'\n",
    "I_loc =r'C:\\Users\\admin\\anaconda3\\01 PROJECTS\\02 HEART RATE'\n",
    "for i in range(len(dataSource)):\n",
    "    clean_ACCDDS(dataSource[i], I_loc, O_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "t='Time'\n",
    "h='HRV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#____Borrowed from FileName:HRV_NN.ipynb this function imports the desired output HRV data\n",
    "def import_HRV(HR_source, timeStamp_S, timeStamp_E):\n",
    "    HRV =0\n",
    "    Running_Sum_HR =0\n",
    "    HRV_sampling_iter =0\n",
    "    prev_HR =0\n",
    "    HR_sampling_interval =10\n",
    "    dfs_HRV =[pandas.DataFrame()]\n",
    "    while(timeStamp_S < timeStamp_E):\n",
    "        file = Path(HR_source+str(timeStamp_S)+\".csv\")\n",
    "        if file.is_file():\n",
    "            with open(HR_source+str(timeStamp_S)+\".csv\", mode='r') as csv_file:\n",
    "\n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                line_count =0\n",
    "                df_HR = pandas.DataFrame()\n",
    "                df_HRV = pandas.DataFrame()\n",
    "                HRV_sampling_iter=0\n",
    "                Running_Sum_HR=0\n",
    "                for row in csv_reader:\n",
    "                    if(line_count >0):\n",
    "                        df_HR = df_HR.append({t:float(row[0]), 'HR':float(row[1])}, ignore_index=True)\n",
    "                    if(HRV_sampling_iter>1):\n",
    "                        Running_Sum_HR = Running_Sum_HR + pow( ( 1/(float(row[1]))-1/(prev_HR) ), 2)\n",
    "                    \n",
    "                        Time_interval = float(row[0]) - df_HR.iloc[line_count-HRV_sampling_iter]['Time']\n",
    "                    #note: large gaps in data, such as HR taken on different days, need to be removed\n",
    "                    #hours to minutes\n",
    "                        Time_interval_hours = (Time_interval-Time_interval%100)*60/100\n",
    "                    #parse the minutes\n",
    "                        Time_interval_minutes = Time_interval%100\n",
    "                    \n",
    "                        HRV =60000*pow((Running_Sum_HR)/HRV_sampling_iter, 0.5)\n",
    "                   \n",
    "                        if(Time_interval_hours + Time_interval_minutes > HR_sampling_interval):\n",
    "                        #To account for sometimes irregular measurements a normalizing factor is used\n",
    "                            n_factor = HR_sampling_interval/(Time_interval_hours + Time_interval_minutes)\n",
    "                            HRV = HRV*n_factor\n",
    "                            df_HRV = df_HRV.append({t:float(row[0]),h:HRV}, ignore_index=True )\n",
    "                            HRV_sampling_iter=0\n",
    "                            Running_Sum_HR=0\n",
    "                    if(line_count>0): \n",
    "                        prev_HR = float(row[1])\n",
    "                    HRV_sampling_iter +=1\n",
    "                    line_count +=1\n",
    "            \n",
    "    #df_HR.plot(kind='scatter', x='Time', y='HR')\n",
    "    #df_HRV.plot(kind='scatter', x='Time', y='HRV')\n",
    "            dfs_HRV.append(df_HRV)\n",
    "    #print(df_HRV['HRV'].describe())\n",
    "        timeStamp_S +=1\n",
    "    return dfs_HRV\n",
    "\n",
    "#____Main______\n",
    "TimeStamp_S =20200711.0\n",
    "TimeStamp_E =20200728.0\n",
    "hr_source =r\"C:\\Users\\admin\\anaconda3\\01 PROJECTS\\02 HEART RATE\\By Day\\Heart Rate\"\n",
    "HrV_dF =import_HRV(hr_source, TimeStamp_S, TimeStamp_E) #HrV_dF[n] n>0 now being continuous HRV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez = []\n",
    "#print(HrV_dF[9].iloc[2][t])\n",
    "for i in range(len(HrV_dF)-1):\n",
    "     if( i>0 and i!=5 and i!=10 and i!=11):\n",
    "        ez += [[HrV_dF[i].iloc[2][t], HrV_dF[i].mean()[h]]]\n",
    "#print(HrV_dF)\n",
    "#print(ez.loc[1][t])\n",
    "#print(ez[t][1])\n",
    "#print(ez[h][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Time        HRV\n",
      "0   2.020071e+11  [0, 0, 1]\n",
      "1   2.020071e+11  [0, 0, 0]\n",
      "2   2.020071e+11  [1, 0, 1]\n",
      "3   2.020071e+11  [0, 1, 1]\n",
      "4   2.020072e+11  [1, 1, 0]\n",
      "5   2.020072e+11  [1, 1, 1]\n",
      "6   2.020072e+11  [1, 0, 0]\n",
      "7   2.020072e+11  [1, 1, 1]\n",
      "8   2.020072e+11  [1, 1, 0]\n",
      "9   2.020072e+11  [0, 1, 0]\n",
      "10  2.020073e+11  [0, 1, 1]\n",
      "11  2.020073e+11  [1, 0, 0]\n",
      "[[202007111259.0, 30.076129026553115], [202007122002.0, 18.247410324976975], [202007131500.0, 82.35622110302852], [202007141800.0, 60.169607333779986], [202007161200.0, 84.65071310503725], [202007171911.0, 128.58312355506555], [202007182100.0, 68.73806843094921], [202007192055.0, 234.33599356691604], [202007232021.0, 126.30310381824934], [202007241850.0, 35.61407227087137], [202007251104.0, 45.28195915913766], [202007261500.0, 66.1469573254955]]\n"
     ]
    }
   ],
   "source": [
    "#This function encodes a dataframe of HRV data in 3 binary by time\n",
    "def encode_HRV(ez):\n",
    "    SCALE = [18.5,30.5,45,60.5,70.5,82.5,127]\n",
    " \n",
    "    for i in range(len(ez)):\n",
    "        if(ez[i][1]<=SCALE[0]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,0,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,0,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[0] and ez[i][1]<=SCALE[1]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,0,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,0,1]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[1] and ez[i][1]<=SCALE[2]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,1,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,1,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[2] and ez[i][1]<=SCALE[3]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,1,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,1,1]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[3] and ez[i][1]<=SCALE[4]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,0,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,0,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[4] and ez[i][1]<=SCALE[5]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,0,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,0,1]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[5] and ez[i][1]<=SCALE[6]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,1,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,1,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[6]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,1,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,1,1]},ignore_index=True)\n",
    "    return df_out\n",
    "\n",
    "#_______Main____\n",
    "e=encode_HRV(ez)\n",
    "print(e)\n",
    "print(ez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1], [0, 0, 0], [1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 1, 1], [1, 0, 0], [1, 1, 1], [1, 1, 0], [0, 1, 0], [0, 1, 1], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "e_e = [e[h][0]]\n",
    "for i in range(len(e[h]) -1):\n",
    "    e_e += [e[h][i+1]]\n",
    "print(e_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#run this script for each of the days ez[1][0] to ez[6][0]\n",
    "def search_and_import_means(time_Search):    \n",
    "    #time_Search = ez[0][0]\n",
    "    time_Search_stamp = (time_Search-time_Search%10000)/10000\n",
    "    readFile_location =r\"C:\\Users\\admin\\anaconda3\\01 Projects\\02 HEART RATE\\By Day\"\n",
    "    total = []\n",
    "    for j in range(len(dataSource)):\n",
    "        data_count = 0\n",
    "        file = Path(readFile_location+dataSource[j]+str(time_Search_stamp)+\".csv\")\n",
    "        if file.is_file():\n",
    "            with open(readFile_location+dataSource[j]+str(time_Search_stamp)+\".csv\", mode='r') as csv_file:\n",
    "    \n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                line_count =0\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    if(line_count>0):\n",
    "                        data_count += float(row[1])\n",
    "                    line_count +=1\n",
    "                \n",
    "        else:\n",
    "            print(\"either file DNE or read error\")\n",
    "            data_count =0\n",
    "        total += [data_count]\n",
    "    return total      #Set the input variable to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day = []\n",
    "for i in range(len(ez)):\n",
    "    Day += [search_and_import_means(ez[1][0])]\n",
    "\n",
    "\n",
    "#Example\n",
    "# Day = [[23.74900000000001, 0, 0.2353067468867503, 538.2353067468867]]\n",
    "# Day += [[21.492, 0, 0.21240253294534364, 432.21240253294536]]\n",
    "# Day += [[12.974000000000004, 13.15201776887913, 13.42767002460025, 628.4276700246003]]\n",
    "# Day += [[1.446, 0, 0.5390392458158961, 1153.539039245816]]\n",
    "# Day += [[10.321000000000002, 0, 1.604717656151466, 2595.6047176561515]]\n",
    "# Day += [[203.24399999999974, 204.33408859576434, 207.74473881057656, 4716.744738810577]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0]]\n"
     ]
    }
   ],
   "source": [
    "print(Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#expected_output = np.array([[0],[1],[1],[0]])\n",
    "#inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "#print([[0],[1],[1],[0]])\n",
    "#print(expected_output)\n",
    "#print([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "#Expected_Output =[ [test1_day1.loc[0]['HRV'][0],test1_day1.loc[0]['HRV'][1],test1_day1.loc[0]['HRV'][2]] ]\n",
    "#for i in range(len(test1_day1)):\n",
    "    #if i>0:\n",
    "        #Expected_Output.append([test1_day1.loc[i]['HRV'][0],test1_day1.loc[i]['HRV'][1],test1_day1.loc[i]['HRV'][2]])\n",
    "#EXPECTED_OUTPUT=np.array(Expected_Output)\n",
    "#print(EXPECTED_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [0.24142852 0.04857911 0.80902157 0.508495  ] [0.99451326 0.77077555 0.31365468 0.27697524] [0.80307571 0.32690299 0.84204035 0.78549596] [0.55614912 0.97304042 0.73357423 0.64560164]\n",
      "Initial hidden biases: [0.7735445  0.98566152 0.10406607 0.16179206]\n",
      "Initial output weights: [0.54044541 0.31494552 0.13945043] [0.40422909 0.83063825 0.83901639] [0.68030961 0.42597723 0.39382609] [0.21304316 0.28874328 0.9355975 ]\n",
      "Initial output biases: [0.00846572 0.20482427 0.86495245]\n",
      "Final hidden weights: [0.24142852 0.04857911 0.80902157 0.508495  ] [0.99451326 0.77077555 0.31365468 0.27697524] [0.80307571 0.32690299 0.84204035 0.78549596] [0.55614912 0.97304042 0.73357423 0.64560164]\n",
      "Final hidden bias: [0.7735445  0.98566152 0.10406607 0.16179206]\n",
      "Final output weights: [ 0.23844126 -0.03078574 -0.49511815] [0.10222494 0.48490699 0.20444782] [ 0.37830546  0.08024597 -0.24074248] [-0.08896099 -0.05698798  0.30102893]\n",
      "Final output bias: [-0.29353843 -0.14090699  0.23038388]\n",
      "\n",
      "Output from neural network after 10,000 epochs: [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ] [0.58333333 0.58333333 0.5       ]\n",
      "[[0, 0, 1], [0, 0, 0], [1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 1, 1], [1, 0, 0], [1, 1, 1], [1, 1, 0], [0, 1, 0], [0, 1, 1], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "#np.random.seed(0)\n",
    "\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Input datasets\n",
    "inputs = np.array(Day)\n",
    "expected_output = np.array(e_e)\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.001\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 4,4,3 #originally 2,2,1\n",
    "\n",
    "#Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
    "hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1,outputLayerNeurons))\n",
    "\n",
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "\n",
    "#Training algorithm\n",
    "for _ in range(epochs):\n",
    "    #Forward Propagation\n",
    "    hidden_layer_activation = np.dot(inputs,hidden_weights)\n",
    "    hidden_layer_activation += hidden_bias\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    output_layer_activation = np.dot(hidden_layer_output,output_weights)\n",
    "    output_layer_activation += output_bias\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "    #Backpropagation\n",
    "    error = expected_output - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    #Updating Weights and Biases\n",
    "    output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "    output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr\n",
    "    hidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "    hidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "\n",
    "print(\"Final hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
    "print(*predicted_output)\n",
    "print(e_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The issue of undertraining the network is exemplified above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
