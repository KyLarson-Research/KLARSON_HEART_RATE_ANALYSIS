{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kyle Larson\n",
    "# File Name: OVERTRAINED.ipynb\n",
    "# License: GPL 3\n",
    "# Objective: The objective of this document is to demonstrate a simple neural network\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import pandas\n",
    "import re\n",
    "\n",
    "#This function converts date and time from 'dd-Mth-yyyy hh:mm' to 'yyyymmddhhmm'\n",
    "def integer_Date_Time(string_1):\n",
    "    months = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    string_2 =re.search(r'\\d+', string_1).group()\n",
    "    integer_date_time =int(string_2)*10000\n",
    "    for i in range(3):\n",
    "        string_1 = string_1.replace(string_2,'')\n",
    "        if(re.search(r'\\d+', string_1)):\n",
    "            string_2 = re.search(r'\\d+', string_1).group()\n",
    "        if i==0:\n",
    "            integer_date_time +=int(string_2)*100000000\n",
    "        if i==1:\n",
    "            integer_date_time +=int(string_2)*100\n",
    "        if i==2:\n",
    "            integer_date_time +=int(string_2)\n",
    "            regex = r'(?<=-)([\\w\\.-]+)- :(\\d+)'\n",
    "            match = re.search(regex, string_1)\n",
    "            if match: \n",
    "                for j in range(len(months)):\n",
    "                    if(re.match(match.group(1), months[j])):\n",
    "                        integer_date_time+=(j+1)*1000000\n",
    "    return integer_date_time\n",
    "\n",
    "#This function cleans Active Calories, Cycling Distance, Distance and Steps files\n",
    "#reading their raw data and writing it to smaller tidier files by day\n",
    "def clean_ACCDDS(data_source, i_loc, o_loc):\n",
    "    d ={ 'Time':[0] , data_source:[0] }\n",
    "    df = pandas.DataFrame(d, columns=['Time', data_source])\n",
    "    \n",
    "    T ='Time'\n",
    "    D =data_source\n",
    "    line_count =0\n",
    "    start_file_index =0\n",
    "    Nextdate =0\n",
    "    date =0\n",
    "    with open(i_loc+data_source+\".csv\", mode='r') as csv_file:\n",
    "    \n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count =0\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if(line_count>0):\n",
    "                df = df.append({T:integer_Date_Time(row[1]),D:float(row[2])}, ignore_index=True)\n",
    "                Nextdate = df.loc[line_count][0] - (df.loc[line_count][0])%10000\n",
    "                #print(str(integer_Date_Time(row[1]))+\", \"+row[2])\n",
    "                if(line_count>1 and Nextdate-date >= 10000):\n",
    "                    with open(o_loc+data_source+str(Nextdate/10000)+\".csv\", mode='w') as outf:\n",
    "                        i =start_file_index\n",
    "                        outf.write(\"Time, \"+data_source+\"\\n\")\n",
    "                        while(i<line_count):\n",
    "                            outf.write(str(df.loc[i][0])+\", \"+str(df.loc[i][1])+\"\\n\")\n",
    "                            i +=1\n",
    "                    start_file_index = line_count\n",
    "            line_count +=1\n",
    "            date = Nextdate\n",
    "    return\n",
    "\n",
    "\n",
    "#___MAIN____\n",
    "dataSource = ['\\Active Calories', '\\Cycling Distance', '\\Distance', '\\Steps']#['\\Heart Rate']\n",
    "O_loc =r'C:\\Users\\admin\\anaconda3\\01 PROJECTS\\02 HEART RATE\\By Day'\n",
    "I_loc =r'C:\\Users\\admin\\anaconda3\\01 PROJECTS\\02 HEART RATE'\n",
    "for i in range(len(dataSource)):\n",
    "    clean_ACCDDS(dataSource[i], I_loc, O_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t='Time'\n",
    "h='HRV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#____Borrowed from FileName:HRV_NN.ipynb this function imports the desired output HRV data\n",
    "def import_HRV(HR_source, timeStamp_S, timeStamp_E):\n",
    "    HRV =0\n",
    "    Running_Sum_HR =0\n",
    "    HRV_sampling_iter =0\n",
    "    prev_HR =0\n",
    "    HR_sampling_interval =10\n",
    "    dfs_HRV =[pandas.DataFrame()]\n",
    "    while(timeStamp_S < timeStamp_E):\n",
    "        file = Path(HR_source+str(timeStamp_S)+\".csv\")\n",
    "        if file.is_file():\n",
    "            with open(HR_source+str(timeStamp_S)+\".csv\", mode='r') as csv_file:\n",
    "\n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                line_count =0\n",
    "                df_HR = pandas.DataFrame()\n",
    "                df_HRV = pandas.DataFrame()\n",
    "                HRV_sampling_iter=0\n",
    "                Running_Sum_HR=0\n",
    "                for row in csv_reader:\n",
    "                    if(line_count >0):\n",
    "                        df_HR = df_HR.append({t:float(row[0]), 'HR':float(row[1])}, ignore_index=True)\n",
    "                    if(HRV_sampling_iter>1):\n",
    "                        Running_Sum_HR = Running_Sum_HR + pow( ( 1/(float(row[1]))-1/(prev_HR) ), 2)\n",
    "                    \n",
    "                        Time_interval = float(row[0]) - df_HR.iloc[line_count-HRV_sampling_iter]['Time']\n",
    "                    #note: large gaps in data, such as HR taken on different days, need to be removed\n",
    "                    #hours to minutes\n",
    "                        Time_interval_hours = (Time_interval-Time_interval%100)*60/100\n",
    "                    #parse the minutes\n",
    "                        Time_interval_minutes = Time_interval%100\n",
    "                    \n",
    "                        HRV =60000*pow((Running_Sum_HR)/HRV_sampling_iter, 0.5)\n",
    "                   \n",
    "                        if(Time_interval_hours + Time_interval_minutes > HR_sampling_interval):\n",
    "                        #To account for sometimes irregular measurements a normalizing factor is used\n",
    "                            n_factor = HR_sampling_interval/(Time_interval_hours + Time_interval_minutes)\n",
    "                            HRV = HRV*n_factor\n",
    "                            df_HRV = df_HRV.append({t:float(row[0]),h:HRV}, ignore_index=True )\n",
    "                            HRV_sampling_iter=0\n",
    "                            Running_Sum_HR=0\n",
    "                    if(line_count>0): \n",
    "                        prev_HR = float(row[1])\n",
    "                    HRV_sampling_iter +=1\n",
    "                    line_count +=1\n",
    "            \n",
    "    #df_HR.plot(kind='scatter', x='Time', y='HR')\n",
    "    #df_HRV.plot(kind='scatter', x='Time', y='HRV')\n",
    "            dfs_HRV.append(df_HRV)\n",
    "    #print(df_HRV['HRV'].describe())\n",
    "        timeStamp_S +=1\n",
    "    return dfs_HRV\n",
    "\n",
    "#____Main______\n",
    "TimeStamp_S =20200711.0\n",
    "TimeStamp_E =20200728.0\n",
    "hr_source =r\"C:\\Users\\admin\\anaconda3\\01 PROJECTS\\02 HEART RATE\\By Day\\Heart Rate\"\n",
    "HrV_dF =import_HRV(hr_source, TimeStamp_S, TimeStamp_E) #HrV_dF[n] n>0 now being continuous HRV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.076129026553115\n",
      "18.247410324976975\n",
      "82.35622110302852\n",
      "60.169607333779986\n",
      "84.65071310503725\n",
      "68.73806843094921\n",
      "234.33599356691604\n",
      "35.61407227087137\n"
     ]
    }
   ],
   "source": [
    "ez = []\n",
    "#print(HrV_dF[9].iloc[2][t])\n",
    "i = 0\n",
    "# for i in range(len(HrV_dF)-5):\n",
    "#     if( i>0 and i!=5 and i!=10 and i!=11):\n",
    "#         ez += [[HrV_dF[i].iloc[2][t], HrV_dF[i].mean()['HRV']]]\n",
    "\n",
    "print(HrV_dF[1].mean()['HRV'])\n",
    "ez += [[HrV_dF[1].iloc[2][t], HrV_dF[1].mean()['HRV']]]\n",
    "print(HrV_dF[2].mean()['HRV'])\n",
    "ez += [[HrV_dF[2].iloc[2][t], HrV_dF[2].mean()['HRV']]]\n",
    "print(HrV_dF[3].mean()['HRV'])\n",
    "ez += [[HrV_dF[3].iloc[2][t], HrV_dF[3].mean()['HRV']]]\n",
    "print(HrV_dF[4].mean()['HRV'])\n",
    "ez += [[HrV_dF[4].iloc[2][t], HrV_dF[4].mean()['HRV']]]\n",
    "# print(HrV_dF[5].mean())\n",
    "# ez += [[HrV_dF[5].iloc[2][t], HrV_dF[5].mean()['HRV']]]\n",
    "print(HrV_dF[6].mean()['HRV'])\n",
    "ez += [[HrV_dF[6].iloc[2][t], HrV_dF[6].mean()['HRV']]]\n",
    "print(HrV_dF[7].mean()['HRV'])\n",
    "ez += [[HrV_dF[7].iloc[2][t], HrV_dF[7].mean()['HRV']]]\n",
    "print(HrV_dF[8].mean()['HRV'])\n",
    "ez += [[HrV_dF[8].iloc[2][t], HrV_dF[8].mean()['HRV']]]\n",
    "# print(HrV_dF[9].mean()['HRV'])\n",
    "# ez += [[HrV_dF[9].iloc[2][t], HrV_dF[9].mean()['HRV']]]\n",
    "# print(HrV_dF[10].mean()['HRV'])\n",
    "# ez += [[HrV_dF[10].iloc[2][t], HrV_dF[10].mean()['HRV']]]\n",
    "print(HrV_dF[11].mean()['HRV'])\n",
    "ez += [[HrV_dF[11].iloc[2][t], HrV_dF[11].mean()['HRV']]]\n",
    "# print(len(HrV_dF))\n",
    "#print(ez.loc[1][t])\n",
    "#print(ez[t][1])\n",
    "#print(ez[h][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Time        HRV\n",
      "0  2.020071e+11  [0, 0, 1]\n",
      "1  2.020071e+11  [0, 0, 0]\n",
      "2  2.020071e+11  [1, 0, 1]\n",
      "3  2.020071e+11  [0, 1, 1]\n",
      "4  2.020072e+11  [1, 1, 0]\n",
      "5  2.020072e+11  [1, 0, 0]\n",
      "6  2.020072e+11  [1, 1, 1]\n",
      "7  2.020072e+11  [0, 1, 0]\n",
      "[[202007111259.0, 30.076129026553115], [202007122002.0, 18.247410324976975], [202007131500.0, 82.35622110302852], [202007141800.0, 60.169607333779986], [202007161200.0, 84.65071310503725], [202007182100.0, 68.73806843094921], [202007192055.0, 234.33599356691604], [202007241850.0, 35.61407227087137]]\n"
     ]
    }
   ],
   "source": [
    "#This function encodes a dataframe of HRV data in 3 binary by time\n",
    "def encode_HRV(ez):\n",
    "    SCALE = [18.5,30.5,45,60.5,70.5,82.5,127]\n",
    " \n",
    "    for i in range(len(ez)):\n",
    "        if(ez[i][1]<=SCALE[0]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,0,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,0,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[0] and ez[i][1]<=SCALE[1]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,0,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,0,1]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[1] and ez[i][1]<=SCALE[2]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,1,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,1,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[2] and ez[i][1]<=SCALE[3]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[0,1,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[0,1,1]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[3] and ez[i][1]<=SCALE[4]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,0,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,0,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[4] and ez[i][1]<=SCALE[5]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,0,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,0,1]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[5] and ez[i][1]<=SCALE[6]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,1,0]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,1,0]},ignore_index=True)\n",
    "        if(ez[i][1]>SCALE[6]):\n",
    "            if(i == 0): df_out = pandas.DataFrame({t:ez[i][0],h:[[1,1,1]]})\n",
    "            else: df_out = df_out.append({t:ez[i][0],h:[1,1,1]},ignore_index=True)\n",
    "    return df_out\n",
    "\n",
    "#_______Main____\n",
    "e=encode_HRV(ez)\n",
    "print(e)\n",
    "print(ez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1], [0, 0, 0], [1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 0, 0], [1, 1, 1], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "e_e = [e[h][0]]\n",
    "for i in range(len(e[h]) -1):\n",
    "    e_e += [e[h][i+1]]\n",
    "print(e_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#run this script for each of the days ez[1][0] to ez[6][0]\n",
    "def search_and_import_totals(time_Search):    \n",
    "    #time_Search = ez[0][0]\n",
    "    time_Search_stamp = (time_Search-time_Search%10000)/10000 \n",
    "    readFile_location =r\"C:\\Users\\admin\\anaconda3\\01 Projects\\02 HEART RATE\\By Day\"\n",
    "    total = []\n",
    "    for j in range(len(dataSource)):\n",
    "        data_count = 0\n",
    "        file = Path(readFile_location+dataSource[j]+str(time_Search_stamp)+\".csv\")\n",
    "        if file.is_file():\n",
    "            with open(readFile_location+dataSource[j]+str(time_Search_stamp)+\".csv\", mode='r') as csv_file:\n",
    "    \n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                line_count =0\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    if(line_count>0):\n",
    "                        data_count += float(row[1])\n",
    "                    line_count +=1\n",
    "                \n",
    "        else:\n",
    "            print(\"either file DNE or read error\")\n",
    "            data_count =0\n",
    "        total += [data_count]\n",
    "    return total      #Set the input variable to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "either file DNE or read error\n",
      "either file DNE or read error\n",
      "either file DNE or read error\n",
      "either file DNE or read error\n",
      "either file DNE or read error\n",
      "either file DNE or read error\n",
      "either file DNE or read error\n"
     ]
    }
   ],
   "source": [
    "Day = []\n",
    "#for i in range(8):\n",
    "Day += [search_and_import_totals(e[t][0])]\n",
    "Day += [search_and_import_totals(e[t][1])]\n",
    "Day += [search_and_import_totals(e[t][2])]\n",
    "Day += [search_and_import_totals(e[t][3])]\n",
    "Day += [search_and_import_totals(e[t][4])]\n",
    "Day += [search_and_import_totals(e[t][5])]\n",
    "Day += [search_and_import_totals(e[t][6])]\n",
    "Day += [search_and_import_totals(e[t][7])]\n",
    "#Example\n",
    "# Day = [[23.74900000000001, 0, 0.2353067468867503, 538.2353067468867]]\n",
    "# Day += [[21.492, 0, 0.21240253294534364, 432.21240253294536]]\n",
    "# Day += [[12.974000000000004, 13.15201776887913, 13.42767002460025, 628.4276700246003]]\n",
    "# Day += [[1.446, 0, 0.5390392458158961, 1153.539039245816]]\n",
    "# Day += [[10.321000000000002, 0, 1.604717656151466, 2595.6047176561515]]\n",
    "# Day += [[203.24399999999974, 204.33408859576434, 207.74473881057656, 4716.744738810577]]\n",
    "# Note that it is okay if the there were no data for certain days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.492, 0, 0.21240253294534364, 432.0], [12.974000000000004, 0.17801776887912596, 0.2756522557211205, 615.0], [1.446, 0, 0.5390392458158961, 1153.0], [10.321000000000002, 0, 1.604717656151466, 2594.0], [31.677000000000007, 0, 0.4880778334975541, 1044.0], [13.835999999999999, 0, 0.5610160852299434, 3801.0], [4.022, 0, 0.21906766339423253, 498.0], [34.59999999999999, 0, 0.33610561473616707, 454.0]]\n"
     ]
    }
   ],
   "source": [
    "print(Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#expected_output = np.array([[0],[1],[1],[0]])\n",
    "#inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "#print([[0],[1],[1],[0]])\n",
    "#print(expected_output)\n",
    "#print([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "#Expected_Output =[ [test1_day1.loc[0]['HRV'][0],test1_day1.loc[0]['HRV'][1],test1_day1.loc[0]['HRV'][2]] ]\n",
    "#for i in range(len(test1_day1)):\n",
    "    #if i>0:\n",
    "        #Expected_Output.append([test1_day1.loc[i]['HRV'][0],test1_day1.loc[i]['HRV'][1],test1_day1.loc[i]['HRV'][2]])\n",
    "#EXPECTED_OUTPUT=np.array(Expected_Output)\n",
    "#print(EXPECTED_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [0.94810991 0.02088211 0.51932745 0.64965537] [0.94936673 0.21179569 0.56289019 0.41134346] [0.39961941 0.80468082 0.74118842 0.4612785 ] [0.81850404 0.31918253 0.1904146  0.48811957]\n",
      "Initial hidden biases: [0.96321143 0.00646563 0.02636592 0.40474956]\n",
      "Initial output weights: [0.13249069 0.95359746 0.27448873] [0.29330172 0.10367821 0.9667121 ] [0.76354999 0.02373785 0.5800546 ] [0.21706182 0.6136592  0.35206536]\n",
      "Initial output biases: [0.45398015 0.55797352 0.49245469]\n",
      "Final hidden weights: [0.94810991 0.02088211 0.51932745 0.64965537] [0.94936673 0.21179569 0.56289019 0.41134346] [0.39961941 0.80468082 0.74118842 0.4612785 ] [0.81850404 0.31918253 0.1904146  0.48811957]\n",
      "Final hidden bias: [0.96321143 0.00646563 0.02636592 0.40474956]\n",
      "Final output weights: [-0.23958618  0.50306821 -0.25866636] [-0.07877515 -0.34685104  0.433557  ] [ 0.39147311 -0.42679139  0.0468995 ] [-0.15501506  0.16312995 -0.18108974]\n",
      "Final output bias: [ 0.08190328  0.10744427 -0.0407004 ]\n",
      "\n",
      "Output from neural network after 10000 epochs: [0.5 0.5 0.5] [0.5 0.5 0.5] [0.5 0.5 0.5] [0.5 0.5 0.5] [0.5 0.5 0.5] [0.5 0.5 0.5] [0.5 0.5 0.5] [0.5 0.5 0.5]\n",
      "[[0, 0, 1], [0, 0, 0], [1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 0, 0], [1, 1, 1], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "#from math import atan\n",
    "#np.random.seed(0)\n",
    "\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Input datasets\n",
    "inputs = np.array(Day)\n",
    "expected_output = np.array(e_e)\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.001\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 4,4,3 #originally 2,2,1\n",
    "\n",
    "#Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
    "hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1,outputLayerNeurons))\n",
    "\n",
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "\n",
    "#Training algorithm\n",
    "for _ in range(epochs):\n",
    "    #Forward Propagation\n",
    "    hidden_layer_activation = np.dot(inputs,hidden_weights)\n",
    "    hidden_layer_activation += hidden_bias\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    output_layer_activation = np.dot(hidden_layer_output,output_weights)\n",
    "    output_layer_activation += output_bias\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "    #Backpropagation\n",
    "    error = expected_output - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    #Updating Weights and Biases\n",
    "    output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "    output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr\n",
    "    hidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "    hidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "\n",
    "print(\"Final hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"\\nOutput from neural network after \"+str(epochs)+\" epochs: \",end='')\n",
    "print(*predicted_output)\n",
    "print(e_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
